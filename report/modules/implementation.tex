\chapter{Implementation}\label{ch:implementation}

\section{Introduction}
For this project, we opted to use Python (version 3.11.1) and applied object-oriented programming principles to modularize our implementation of our library, which provides a single interface for accessing the following cloud storage services: Dropbox, Box, Google Drive and Amazon S3.

We used the official \acp{sdk} offered by each service provider (details shown in \autoref{tab:SDKs}). To facilitate implementation of a common interface across all providers, we used an abstract class which includes methods for each major functionality, namely the upload, download and deletion of items. We then captured every provider in their own class, extending the abstract class for each one. When running our application, the user needs to provide command-line arguments which specify: 1) the cloud service they want to access, 2) the desired functionality (upload/download/delete), 3) a remote path (absolute path of an item stored in the cloud) and, sometimes, 4) a local path (absolute path of a locally stored item). To ensure enhanced reliability and to allow for better monitoring of progress, we used chunked uploading for  files larger than 100MB.


\begin{table}[!h]
    \centering
     \caption{SDKs Used} 
      \label{tab:SDKs}
    \begin{tabular}{|l|l|l|}
        \hline
        \rowcolor[HTML]{EFEFEF}
        \multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}\textbf{Cloud Storage Service}} &
        \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{Python Package Name}} &
        {\color[HTML]{333333} \textbf{Version}} \\ \hline
        Dropbox                & Dropbox SDK       & 11.36.0 \\ \hline
        Box                    & Box SDK           & 3.6.1   \\ \hline
        Google Drive           & Google API Client & 2.78.0  \\ \hline
        Amazon S3              & Boto3             & 1.26.30 \\ \hline
    \end{tabular}

\end{table}


Additionally, to track events that occur during program execution, we used Python's logging standard module. Compared to the standard print function, the logging module allows for greater control over how the messages will be formatted, where the events should be logged (e.g., console or file) and what events should be logged, based on their severity. The importance of events is categorized as shown in \autoref{tab:Logging_Levels}, and only events of the specified severity and above are logged during execution, while the rest are suppressed~\cite{python_logging}. The logging module is also used by all the SDKs we used.

\begin{table}[!h]
    \centering
    \caption{Logging Event Levels}
    \label{tab:Logging_Levels}
    \begin{tabular}{|l|l|}
        \hline
        \rowcolor[HTML]{EFEFEF}
        \textbf{Level}    & \textbf{Used for}                \\ \hline
        \textit{DEBUG}    & Information with great detail, typically to diagnose problems                \\ \hline
        \textit{INFO}     & Confirmation of normal operation \\ \hline
        \textit{WARNING}  & Unexpected events or problems that will soon occur                           \\ \hline
        \textit{ERROR}    & Serious problems that lead to the software not performing some functionality \\ \hline
        \textit{CRITICAL} & Serious errors which may lead to interruption of software execution          \\ \hline
    \end{tabular}
\end{table}

Finally, our library is cross-platform compatible, working well across Windows 10, macOS and Linux, as it takes into account the default file separator of the OS that is running the library. All software that were used or accounts that were created for the purposes of this project were free of charge.

\section{Dropbox Implementation}
The Dropbox Python SDK provided functionality for all major Dropbox features including the creation and management (uploading, rename, requesting, versioning, tagging etc.) of files and folders, Dropbox Papers, shared links and shared folders, and the management of both users and teams~\cite{dropbox_docs}.

Regarding authentication, Dropbox uses OAuth2 and allows for scope configuration, based on what the application needs to access . The SDK can handle the entire OAuth flow on behalf of the application, greatly simplifying the process, and even provides a variation of this flow that does not redirect to a web page in the end, which is ideal for our implementation.  We used a short-lived access token along with a refresh token, and enable \ac{pkce} for increased security, as is recommended for desktop apps~\cite{dropbox_oauth}.

The SDK uses absolute paths to specify Dropbox items, which made it very convenient to work with as most of the other services we implemented require to first find the item (or its parent) and then get the item id or another unique field. Some notable aspects of the SDK when it came to the three major functions we implemented are: ~\cite{dropbox_docs}

\begin{enumerate}
    \item \textbf{\textit{Upload}:} To upload local folder contents, they must be manually enumerated, as the SDK does not do it automatically. When uploading items, the SDK automatically creates any intermediary Dropbox folders specified in the absolute path that do not already exist, hence removing the need to first manually check for and create them. Furthermore, if a file to be uploaded already exists on Dropbox, it is automatically overwritten.

    \item \textbf{\textit{Download}:} Both files and folders can be downloaded, and the latter are downloaded as a zip files for efficiency.

    \item \textbf{\textit{Delete}:} Deletion of files and folder is supported by the Dropbox SDK, and the contents of a folder are deleted along with the folder itself.
\end{enumerate}


\section{Box Implementation}
The Python SDK offered by Box facilitates file and folder manipulation, file requests, versioning and classification, task definition, user and group management, and more~\cite{box_docs}.

For authentication, Box provides a variety of options, namely using OAuth, \ac{jwt}, a Client Credentials Grant or an App Token. From these, however, only OAuth2 with scope-based access control was valid for our case, as the rest involved server-side authentication and were more suited for applications that store data within their own Box account, rather than a user's account.  The Box SDK takes care of most of the OAuth2 flow, but does not provide a variation not involving a redirect, so we had to set up a local server which listens to a local port where the user will be redirected. At the end of the flow, an access token and a refresh token are obtained and stored~\cite{box_auth}.

To specify an item on Box, the SDK does not use an absolute path, but instead requires the item's id. This means that, given the absolute path by the user, we needed to traverse the directory structure manually, ensuring that the entire path is valid (i.e., all intermediary folders and the item at the end exist), before finally retrieving the id of the item. Regarding the major functionalities we implemented: ~\cite{box_docs}


\begin{enumerate}
    \item \textbf{\textit{Upload}:} Like the Dropbox SDK, local folder contents need to be manually enumerated and uploaded, however some additional limitations exist. Specifically, all intermediary folders that do not exist need to first be created, and the upload method offered by the SDK does not overwrite existing files. So, if the upload method is called for an existing file, an error is generated, which means that before uploading any file it becomes necessary to first check for a file's existence, using a separate update method to perform overwrites.

    \item \textbf{\textit{Download}:} Download of folders (as zipped file) and files is supported.

    \item \textbf{\textit{Delete}:} Using the SDK, files and folders can be deleted
\end{enumerate}


\section{Google Drive Implementation}
The Google Workspace API Python client allows for easy use of many Google Drive features like the management of files and folders, file revisions, item permissions, labels and shared drives~\cite{drive_docs}.

Authentication for Google Workspace APIs, including Drive API, is done using service account credentials (for accounts used by applications, uses OAuth2), API keys (for publicly available data) , and OAuth2 for end-user authentication. For our case, OAuth2 with scoped application access was the only valid choice. While there is no OAuth2 flow offered by the Drive API client which does not involve a user redirect, the API client itself handles the entire flow (including setting up a local server to listen for the authorization response) and in the end returns the access/refresh tokens for later use~\cite{drive_auth}.

The Google Drive API client also requires the id of a file or folder to perform operations on it, again making a traversal of the file structure (based on the user-provided absolute path) necessary. Some details regarding the upload, download and delete capabilities of the API client which are worth mentioning: ~\cite{drive_docs}

\begin{enumerate}
    \item \textbf{\textit{Upload}:} Both the limitations regarding the upload of folders  (included files and folders need to be manually located and separately uploaded) and already existing files (a pre-upload check needs to be made, and a separate update method needs to be called) are observed with the Drive API client.

    \item \textbf{\textit{Download}:} In contrast to the other two personal file storage services implemented (Dropbox, Box), the download of folder contents needs to be manually carried out. Specifically, mirroring the upload process, folders themselves cannot be downloaded using the API client, instead requiring a manual download of each file included and the local creation of all subfolders.

    \item \textbf{\textit{Delete}:} Both files and folders can be deleted using the API client.
\end{enumerate}


\section{Amazon S3 Implementation}
Boto3, the Python SDK for \ac{aws},  can be used to manage all versions of buckets and objects, assign \ac{acl}, labels, bucket policies and lifecycles and more, providing access to every major S3 functionality~\cite{s3_docs}.

Amazon recommends using \ac{iam} identity center authentication to manage access to S3 and other AWS services. This process involves creating an AWS Organization and an IAM user to whom you assign least-privilege permissions, so that they are able to programmatically access S3 resources. Then, every time Boto3 needs to access S3 resources, a new IAM Identity Center session is created using the user's credentials, and an access and refresh token is obtained. These tokens are then used by the SDK to initiate a permission set session (IAM session), hence gaining the necessary credentials for AWS access (permission set credentials) by assuming an IAM role. There also exist alternative methods for authentication, such as using IAM role credentials (short-term) or IAM user credentials (long term). However, Amazon advises against these and other methods, as they are not as secure or convenient, and so we opted to go with IAM identity center authentication~\cite{s3_auth}.

To perform S3 operations, Boto3 requires a bucket name and (sometimes) the object key. So, to ensure consistency among all services implemented, we have the user enter the S3 URL of an object (e.g., bucket-name/folder-name/object-name). From the URL, the bucket name can be easily extracted, and then the object key is simply what remains from the URL. So, it is not required to traverse the file structure to find the object specified by the user, as the key itself is sufficient. Regarding the upload, download and deletion of buckets and objects: ~\cite{s3_docs}

 \begin{enumerate}
     \item \textbf{\textit{Upload}:} To upload folder contents, manual enumeration of local files is again required. Even though S3 uses a flat file structure to store objects (i.e., everything is stored inside a single-level bucket), if the key of an object specifies parent folders (e.g., folder1/folder2/object-name), then S3 automatically creates these (symbolic) parent folders when uploading the object. In this way, S3 imitates a hierarchical file structure which helps the users organize their objects, and alleviates the need of manually creating parent folders. Any file upload overwrites existing files, and multipart (chunked) upload is automatically used by Boto3 once a (configurable) threshold is reached.

     \item \textbf{\textit{Download}:} Folder downloads are not offered as a method by Boto3 so, like in the case of Box, all included files need to be manually downloaded and all included folders need to be locally created. Similarly, the download of buckets is not supported, thus requiring a manual download of all bucket contents instead.

     \item \textbf{\textit{Delete}:} Boto3 offers methods for the deletion of both buckets and objects, but the former must first be emptied, which means deleting all included objects. Because S3 folders are symbolic, however, they cannot be directly deleted. Instead, all included objects must be deleted, which also automatically deletes the parent folder.
 \end{enumerate}