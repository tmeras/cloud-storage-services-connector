\chapter{Cloud Storage Services Comparison}\label{ch:cloud-storage-services-comparison}


\section{Introduction}
Cloud storage solutions are rapidly rising in popularity among individuals and enterprises, with the benefits of using such services in day-to-day life and business becoming more and more evident. Cloud storage applications like Google Drive are used by almost 40\% of households in certain areas while, by 2025, 100 \textit{Zettabytes} of data (50\% of all data) will be stored in the cloud~\cite{zeta}.

Customers nowadays have dozens of options when it comes to storage providers. Some of the most popular ones are established providers like Dropbox~\cite{dropbox}, and tech giants such as Amazon and Microsoft, that offer their \ac{s3}~\cite{s3} and Azure Blob services~\cite{blob}, respectively. All storage services, however, differ in certain characteristics like pricing, capacity and performance. It becomes important, then, to investigate these differences in order to make an educated decision on which service to adopt.

The first distinction that needs to be made is between cloud \textit{object} storage and cloud \textit{file} storage~\cite{objectvsfile, objectvsfile2}:

\subsection{Object storage}
This technology allows data to be stored and managed in an unstructured format called \textit{objects}. Objects include the data that make up a file, user-created metadata and a unique identifier. They are stored in a flat data environment which enables fast scaling and application can easily retrieve objects of any data type, using the metadata and the identifier.

As businesses expand, they are tasked with handling larger and larger volumes of data from a variety of sources that is used by both applications and users. This data is often unstructured and in many different formats and storage media, making it hard to store it in a central repository. Therefore, cloud object storage can solve this issue as it offers vast scalability and cost-efficient storage tiers for storing all kinds of data natively in a single virtual repository, accessible from anywhere. An example of cloud object storage is Amazon S3~\cite{s3}.

Some common use cases for object storage are:
\begin{itemize}
    \item[--] \textit{\textbf{Data lakes}}, which are centralized repositories for storing data of any scale, structured or unstructured, and rely on object storage to operate~\cite{datalake}. They offer encryption, access control and great scalability, allowing for easy and dynamic storage expansion of up to petabytes of content with a pay-as-you-go charging model.

    \item[--] The virtually limitless data that is stored in cloud object storage is often used by businesses to perform \textit{\textbf{big data analytics}}, in an effort to better understand their customers, operations and market.

    \item [--] Cloud object storage provide flexible data storage for \textit{\textbf{cloud-native applications}}, accessible via an API, thus aiding developers and allowing for easier and faster deployment.

    \item[--] An excellent use of object storage is \textbf{\textit{data archiving}}, replacing on-site archive infrastructure and providing enhanced data security, durability, accessibility and near-instant retrieval times.

    \item [--] Object storage systems can used to replicate data across many systems, regions and data centers, ensuring that \textbf{\textit{backups}} are ready to be used for recovery from hardware failures.

    \item [--] Replication on a global scale can be achieved with cloud object storage, vastly reducing the storage costs and increasing availability for \textit{\textbf{rich media}} like music, videos and digital images.

    \item [--]  Since \textit{\textbf{\ac{ml}}} models generate inferences after being trained over billions of data items, object storage becomes a necessity to handle such scale in a cost-efficient manner.
\end{itemize}

\subsection{File Storage}
In contrast to object storage, file storage utilizes a hierarchical structure for storing data, where files are stored inside of folders which are grouped into directories and so on, and strict protocols like \ac{nfs} are used. Consequently, it is more difficult to locate a specific piece of data among billions, compared to object storage, where each object is uniquely identified. In addition, the inherent hierarchy and pathing limits the potential scalability of cloud file storage, whereas object storage provides near-limitless scaling.

File synchronization is one of the major features that have contributed to the popularity of these systems. File storage allows users to automatically synchronize their files across a variety of devices, making it very convenient and simple to continue their work as they left it. Another popular use case of file storage collaborative work, as the most up-to-date version of a file can be easily shared among a group of users~\cite{personal1}. The user interface and experience of cloud file storage services are also significantly more user-friendly, making them suitable for casual and experienced users alike. An example of cloud file storage is Dropbox~\cite{dropbox}.


\section{Object Storage Services}
Two of the most popular object storage services are Amazon \ac{s3} and Microsoft Azure Blob Storage.

In Amazon S3, objects are stored in buckets, and prefixes (shared names) are used to organize objects. \textit{S3 object tags}, which are a set of up to 10 key-value pairs, can also be appended to objects to make storage management easier, as these tags can be added and edited at any point in the object's lifetime for stronger access control and other use cases. In addition, \textit{S3 inventory} reports can be generated manually or automatically to provide information on the objects in a bucket, to make working with lots of data easier and to allow gathering information on encryption, replication and more. Amazon provides an online Management Console, a \ac{rest} API, a \ac{cli} and an \ac{sdk} in various languages for accessing S3 features~\cite{s3faq}.

Microsoft Azure Blob Storage uses three types of related resources. The first are storage accounts, which provide a unique namespace for the user's data and is used in the address of each object. There are different kinds of storage accounts, each tailored for different uses and with different redundancy options~\cite{blobaccounts}:

\begin{enumerate}

    \item \textit{General-purpose v2} is the standard account for storing blobs (data). It is recommended for most use cases.

    \item \textit{Premium block blobs}, for block blobs and append blobs. Makes use of \ac{ssd} technology, making it ideal for workloads that require low latency or that involve many transactions.

    \item \textit{Premium page blob}, which is similar to a premium block bob account, but it's only used for page blobs.

    \item \textit{General-purpose v2} is the standard account for storing blobs (data). It is recommended for most use cases.

    \item \textit{Premium block blobs}, for block blobs and append blobs. Makes use of \ac{ssd} technology, making it ideal for workloads that require low latency or that involve many transactions.

    \item \textit{Premium page blob,} which is similar to a premium block bob account, but it's only used for page blobs.

\end{enumerate}

Containers are the second resource, which organize a set of blobs. An unlimited number of containers can be included in a storage account, and an unlimited number of blobs can be stored in a container. The third and final resource kind are blobs, of which there are three types:

\begin{enumerate}
    \item \textit{Block blobs} are made up of blocks of data, with each block being uniquely identified. Block blobs are ideal for large amounts of text or binary data, as up to 50.000 variable size blocks can be included in a single block blob.

    \item \textit{Append blocks} are also made up of blocks of data, but they are optimized for append operations, making them ideal for use cases like \ac{vm} data logging.

    \item \textit{Page blobs}  are composed of 512-byte pages and are suitable for random write and read operations. They support Azure virtual machines by acting as disks.
\end{enumerate}

As with S3, to help the management of large pools of data, Blob storage provides the ability to use \textbf{blob index tags}, key-value pairs that can be used to categorize blobs, find specific blobs across an entire account, set permissions and more. With \textbf{Azure Cognitive Search}, blob content are imported as search documents that are indexed, allowing for the information stored in the blob data themselves to be searched. Also, with the help of \ac{ai} enrichment, text from images can be extracted. Furthermore,  \textbf{Blob inventory} automatically generates report with user-defined rules to provide an overview on containers, snapshots, blobs, blob versions and their properties. Blob storage object are accessible via REST API, Azure PowerShell (cmdlets fo Azure resources)~\cite{powershell}, CLI or a client library in one of many languages.~\cite{blobinfo}

Both S3 and Blob Storage also provide\textbf{ monitoring and logging} for their resources to keep track of their performance, operation, cost efficiency and security and to receive alerts for specified events. S3 has automatic (Cloudwatch Alarms and Cloudtrail Log Monitoring) and manual (S3 dashboard, AWS Trusted advisor and more) monitoring tools, while logging is possible via server access logs (for object-level operations) and API call logs with AWS CloudTrail (for bucket-level and object-level visibility). Azure Blob Storage works with Azure Monitoring (used for all Azure resources) for monitoring and logging purposes. ~\cite{s3monitoring,blobmonitoring} In addition, S3 offers even further enhanced storage visibility via S3 Storage Lens and S3 Storage Class Analysis. The former provides a single view of storage usage and activity patterns across potentially thousands of accounts at various levels and provides actionable recommendations for improving data protection and cost efficiency. At the same time, Storage Class Analysis helps in making the correct storage class transitions by monitoring object access patterns. ~\cite{s3analytics}

\subsection{Storage Classes}
Blob Storage and S3 both offer various storage classes to match the data access patterns and resiliency requirements of different workloads, at the lowest cost possible.

S3 makes the following distinctions regarding its storage classes: ~\cite{s3faq}
\begin{description}
    \item[S3 Standard] is recommended by Amazon for regularly accessed data, offering great performance, availability and durability for data that is regularly accessed. It is characterized by high throughput and low (millisecond) latency, making it ideal for big data analytic, gaming applications, content distribution and many more scenarios. In general, S3 classes allows the customers to make configurations at the object level, and a bucket can contain objects stored across different classes.

    \item [S3 Standard-Infrequent Access (S3 Standard-IA)] is best used for less frequently accessed data, that nonetheless must be rapidly accessed when required. It is identical to standard S3 in terms of latency, throughput and durability, but with low price (in GB storage) and retrieval, meaning that it is suitable for data to be stored long-term, for disaster recovery files and for backups.

    \item[S3 One Zone-Infrequent Access (S3 One Zone-IA)] is identical to S3 Standard-IA, with the difference being that data is stored in one \ac{az}, instead of three like the other storage classes. This has the immediate effect of 20\% lowers costs, but if the single AZ is destroyed, all data will be lost along with it.

    \item[S3 Glacier] classes are tailor-made for archiving purposes, and there are three of them:
    \begin{itemize}
        \item [--] \textit{S3 Glacier Instant Retrieval} offers the lowest cost for storing data long-term which is rarely accessed and need to be retrieved in millisecond. If the data is accessed once every 4 months, around 70\% will be saved in storage costs when compared with S3 Standard-IA. This class is the fastest in accessing archived data, whilst having the same throughput performance and access latency as S3-standard, meaning that it is ideal for medical images, news media content and more data which is archived but need instant access.

        \item[--] \textit{S3 Glacier Flexible Retrieval}, which formerly was S3 Glacier, offers 10\% lower, compared to Glacier Instant Retrieval, costs for archived that is to be accessed once or twice per year and do not require immediate access. It balances costs and access times (can be from hours or minutes), making it ideal for use cases like disaster recovery and backups.

        \item[--] \textit{S3 Glacier Deep Archive} offers the lowest storage costs in Amazon S3, used for retaining and preserving data long-term, which might be accessed 1 or 2 times per year. It offers retrieval times of up to 12 hours, and targets customers that need to retain data for many years to meet regulations, making it especially applicable for healthcare, public sector and other industries. It can also replace magnetic tape systems.
    \end{itemize}

    \item[S3 Intelligent-Tiering] automatically moves data to the access tier such that the costs will be minimized, with no impact in terms of performance, retrieval costs or operation. It charges a small fee every month for monitoring and automatically moving objects, but can lead to immense cost savings. If an objects hasn't been accessed for 30 or 90 days, it is moved automatically to the Infrequent and Archive Instant Access tier, respectively. If the user does not require immediate retrieval, object can be automatically moved to the deep Archive Tier after not being accessed for 180 days. Of course, if an object in one of these tiers is accessed, then it reverts to the frequently accessed tiers, but objects in Deep Archive first need to be restored (three options exists for archive retrieval, with different access times for each). ~\cite{archiverestore} This class is ideal for data with access patterns that are not known or that are unstable.

    \item[S3 Outposts] provides on-premise objects storage, which is useful for data that needs to be stored locally to meet performance, processing or residency requirements, while also taking advantage of S3 features offered on the cloud, as the same APIs are used.
\end{description}

S3 also provides object\textbf{ Lifecycle management}, which allows for defining an object's lifecycle with a policy to automatically migrate objects from the S3 standard class to others, thus saving on storage costs and accommodating different access patterns. Most transitions between classes are supported, but there are exceptions because, for instance, an object can't be transitioned to S3 standard from any other class. Furthermore,  policies can be set to automatically remove objects and incomplete multipart uploads according to their age, which saves in storage costs and time. Lifecycle rules allow for great configuration, as they can be used in conjunction with prefixes to, for example, discard specific objects after a certain number of days has passed. ~\cite{s3lifecycle}

\bigskip
Blob Storage offers its own access tiers: ~\cite{blobinfo}
\begin{description}
    \item[Hot Tier] is used for storing data online that is actively used or that will be accessed regularly. It is the costliest for storing data, but the cheapest for accessing it.

    \item[Cool Tier] also stores data online, but the data is to be accessed less frequently (short-term backups are example use cases). This tier has the same access latency, durability, and throughout performance as the hot tier, but slightly lower availability. It also has higher access costs, but lower storage costs overall, hence why it is ideal for data that isn't very frequently used.

    \item[Archive Tier] is the offline tier offered by by Blob Storage, ideal for storing data that is only rarely accessed. It offers the lowest overall storage costs, but higher latency and retrieval costs when compared to the online tiers. This tier is suitable for scenarios like long-term backups and archival data which is very infrequently accessed. However, it must be mentioned that, depending on redundancy configurations, not all accounts are allowed to move blobs to the archive tier and archived blobs cannot be read or modified, so they must be first rehydrated from the Archive tier. Rehydration is possible by either changing the blob's access tier to either hot or cold, or by copying it to one of those tiers. The time required fo rehydration depends on the priority, as standard priority rehydration may take 15 hours for objects of under 10 GB in size, whereas high priority rehydration could complete in under an hour.
\end{description}

Changing a blob's access tier is possible by manually setting the blob tier or by copying the blob to another tier. The former is recommended as it avoids the early deletion penalty which, in the case of cold tier blobs, incurs a charge if a blob is deleted or moved to a different tier less than 30 days after being created. S3 also has a similar penalty for its classes. Like S3, Blob Storage allows for \textbf{Lifecycle Management}, providing the opportunity to set cost-efficient policies with rules that specify when data should be transitioned to another tier, according to access patterns, and when objects should be expired, according to age.

So, when compared to S3, Azure Blob offers less options in terms of storage classes (e.g. there's no equivalent to S3's Outposts and Intelligent-Tiering), and places more restrictions on archived data.

\subsection{Data Redundancy}
Amazon Simple Storage Service strengthens data redundancy thanks to \textbf{S3 Replication}, for which 3 different options exist: ~\cite{s3replication}
\begin{itemize}
    \item [--] \textit{S3 \ac{crr}} can be used to replicate along with their tag and metadata to other AWS regions. This helps with compliance requirements for storing data at great distances,latency minimization by maintaining data close to customers, disaster recovery and more.

    \item [--] \textit{S3 \ac{srr}} replicates data between buckets, but only within the same region. Example use cases include aggregating data (e.g. logs) in a single location for processing or abiding by legal requirements that require data to be kept in another AWS account in the same region.

    \item [--] \textit{S3 Batch Replication} is unique as it replicate existing objects, instead of newly created ones like CRR and SRR do. So, it complements the other replication methods and can be used for instance to reattempt failed replications or to migrate existing objects to other buckets.
\end{itemize}

Another S3 feature is \textbf{Replication Time Control}, which is backed by a \ac{sla} and claims that, during nay month, 99.99\% of objects will be replicated within 15 minutes, otherwise Service Credits will be provided to the customer as compensation.~\cite{s3replicationsla}

S3 allows replication to happen at a bucket level, a shared prefix level or an object level (using object tags) with multiple destination buckets in the same or different regions and even two-way replication among buckets is possible. On top of all this, \textbf{S3 Multi-Region Access Points} can vastly boost performance (up to 60\%) as it automatically routes requests to the copy with the lower latency, taking into account location,  network congestion etc. It can be used in unison with CRR to allow for a single endpoint for all replicas, hence simplifying management. ~\cite{s3replication}

According to Amazon, S3 storage classes (except Outposts and S3 One Zone-IA) redundantly store data across 3 AZs (at least), thus attaining 11 9's (99.99999999999\%) of objects durability for a given year, meaning that with 10 million objects stored, one will be lost every 10 thousand years. S3 Outposts enforce durability by redundantly storing data across devices within the on-premises data center, while S3 One Zone-IA does the same but within a single AZ.

\bigskip
In the primary region, Azure Blob always replicate data three times and 2 redundancy options are provided:~\cite{blobinfo}
\begin{itemize}
    \item [--] \textit{\ac{lrs}} synchronously copies data in a primary region data center. It is the cheapest but least durable option (at least 11 9s), as if some sort of disaster occurs in the data center all copies will be lost. It could be useful for abiding to requirements that restring data to only a single region, or for data that can be easily reconstructed after loss.

    \item[--] \textit{\ac{zrs}} also synchronously replicates data in the primary region, but across different Azure availability zones (physical locations), hence attaining at least 12 9's of storage durability, so it is a better choice for high durability requirements, although region-wide disasters might still lead to data loss.
\end{itemize}

Microsoft also allows and recommend replicating data to a secondary region far from the primary region in order to survive regional disasters. There exist two options for secondary region redundancy:
\begin{itemize}
    \item [--]  \textit{\ac{grs}} first uses LRS for the primary region, and then asynchronously replicates the data in a secondary region , where again LRS is used. It claims significantly higher durability (at least 16 9's) during a year.

    \item [--] \textit{\ac{gzrs}} is the most robust replication option. It is identical to GRS, with the only (significant) difference being that ZRS is used initially, instead of LRS. This results in the combined benefits of both high availability in the primary zone (ZRS) and protection against a regional disaster (geo-replication). It provides 16 9's of durability or more yearly.
\end{itemize}
It should be noted that data stored in secondary regions is normally only accessible after an outage renders the primary region unavailable, and the asynchronous replication leads to the secondary region being slightly outdated. The option, however, exists to allow reading data in the secondary region via the \ac{ra-grs} or \ac{ra-gzrs} configurations.

In general, compared to S3, Blob Storage could prove to be more affordable via LRS (if high availability and durability is not a major concern) and better durability is promised (16 9's vs 11 9's of S3). However blob storage also has drawbacks, as for example redundancy settings are account-wide while S3 allows for even object-level specification, and there is no SLA backing replication speed as in S3 Replication Time Control.

\subsection{Availability}
With the S3 SLA, a commitment is made by Amazon to maintain a specific uptime percentage for its services every month.\cite{s3sla} If lower than promised availability for a service is observed during a month (exceptions apply for unusual situations like natural disasters), then customers are entitled to Service Credits which can be applied against future payments towards that service. The percentage of charges for a billing cycle that service credits cover is based on table \ref{S3 SLA}.

\begin{table}[h]
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{cll}
            \hline
            \multicolumn{1}{l}{\textbf{Service Credit \%}} &
            \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}
                                           Standard/ Glacier/ \\ Glacier Deep Archive
            \end{tabular}}} &
            \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}
                                           Intelligent-Tiering/ Standard-IA/\\ One Zone-IA/ Glacier Instant Retrieval
            \end{tabular}}}  \\ \hline
            \multicolumn{1}{c|}{10\%}  & \textless 99.9\% and \textgreater{}= 99\% uptime  & \textless 99\% and \textgreater{}= 98\% uptime \\ \hline
            \multicolumn{1}{c|}{25\%} & \textless{}99\% and \textgreater{}= 95\% uptime                           & \textless{}98\% and \textgreater{}= 95\% uptime                          \\ \hline
            \multicolumn{1}{c|}{100\%} & \textless{}95\% uptime                           & \textless{}95\% uptime
        \end{tabular}%
    }\caption{\label{S3 SLA}S3 Availability SLA}
\end{table}

Blob Storage also provides an availability SLA \cite{blobsla}, again offering credits as compensation for lower-than-expected uptime.(table \ref{Blob SLA})
\begin{table}[h]
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{clll}
            \hline
            \multicolumn{1}{l}{\textbf{Service Credit \%}} &
            \multicolumn{1}{c}{\textbf{Hot Blobs}} &
            \multicolumn{1}{c}{\textbf{Cool Blobs}} &
            \multicolumn{1}{c}{\textbf{Archive Blobs}} \\ \hline
            \multicolumn{1}{c|}{10\%} &
            \begin{tabular}[c]{@{}l@{}}
                \textless{}99.9\% write requests \\ in LRS, ZRS, GRS, RA-GRS\\ \\ \textless{}99.99\% read requests\\  in RA-GRS
            \end{tabular} &
            \begin{tabular}[c]{@{}l@{}}
                \textless{}99\% write requests \\ in LRS, GRS, RA-GRS\\ \\ \textless{}99.9\% read requests \\ in RA-GRS
            \end{tabular} &
            \begin{tabular}[c]{@{}l@{}}
                \textless{}99\% write requests \\ in LRS, GRS, RA-GRS\\ \\ \textless{}99.9\% read requests \\ in RA-GRS
            \end{tabular} \\ \hline
            \multicolumn{1}{c|}{25\%} &
            \begin{tabular}[c]{@{}l@{}}
                \textless{}99\% write requests\\ in LRS, ZRS, GRS, RA-GRS\\ \\ \textless{}99\% read requests \\ in RA-GRS
            \end{tabular} &
            \begin{tabular}[c]{@{}l@{}}
                \textless{}98\% write requests\\  in LRS, GRS, RA-GRS\\ \\ \textless{}98\% read requests \\ in RA-GRS
            \end{tabular} &
            \begin{tabular}[c]{@{}l@{}}
                \textless{}98\% write requests \\ in LRS, GRS, RA-GRS\\ \\ \textless{}98\% read requests\\  in RA-GRS
            \end{tabular}
        \end{tabular}%
    }\caption{\label{Blob SLA}Blob Storage Availability SLA}
\end{table}

S3 and Blob Storage promise similar availability, with the main differences being that Blob Storage specifies availability for reads and writes, while S3 offers full (100\% Service Credit) compensation if availability is significantly low.

\subsection{Security}
S3 and Blob Storage both follow strict security standards and they offer lots of support for access control and encryption.

\subsubsection{Data Handling}
For securing \textbf{data in transit}, Amazon recommends using HTTPS (TLS) to prevent eavesdropping , man-in-the-middle and other attacks. Conditions can be set on bucket policies so that only encrypted connections (over HTTPS) are allowed, and a rule can be used to check that all buckets require \ac{ssl} to be used by requests. Client-side encryption is also allowed, which can be achieved in one of two ways: \cite{s3securitypractices,s3clientside,s3vsblobsecurity}

\begin{enumerate}
    \item Using a AWS \ac{kms} key ID, clients can request a unique data key to encrypt an object before uploading it. AWS KMS then sends a plaintext version of the key and its cipher blob which will be included in the object metadata. To download an object, the client downloads the cipher blob and the encrypted object, and then sends the blob to AWS KMS to get the plaintext data key for decryption.

    \item Clients can also generate, use and store their own encryption keys (called root keys), which will be provided to the S3 encryption client to generate and encrypt a one-time symmetric encryption key (data key) that will encrypt the uploaded object. Object metadata is used to determine the root key that will decrypt the data key for a downloaded object. So, it is up to the client here to manage the keys and encryption process, but they can decide what root keys to use (e.g. symmetric), the algorithms etc.
\end{enumerate}

In similar fashion, Blob Storage has the option of requiring API calls to be made over HTTPS and clients can even enforce a minimum TLS version for incoming requests, enhancing overall security. Furthermore, client-side encryption can also be used, which works similarly as it does in S3, the main difference being that encryption keys can be stored either locally or on the Azure Key Vault, which is Azure's personal key management solution. \cite{blob_minimum_tls,blob_clientside,azure_vault,blob_secure_transfer}

\textbf{Data at rest} can be protected in S3 using client-side encryption (as described earlier) and server-side encryption, and there are three mutually-exclusive options for the latter: \cite{s3serverside}

\begin{itemize}
    \item [--] With \textit{\ac{sse-s3}}, each object is encrypted with a unique key which itself is encrypted with a root key that changes frequently. All this is handled by S3. A very strong 256-bit block cipher is used for encryption.

    \item [--] \textit{\ac{sse-kms}} is identical to SSE-S3, but has the added protection that comes with the separate access permissions involving KMS keys, and provides an audit trail for the these keys. SSE-KMS allows the keys to be handled and generated by the user or Amazon S3, and some extra fees are applied.

    \item [--] \textit{\ac{sse-c}} is also similar, but the customer is responsible for providing and securely storing the encryption key, with Amazon handling the encryption/decryption processes.
\end{itemize}

Blob Storage also allows client- and server-side encryption for data at rest, and for the latter it uses strong 256-bit encryption,which is automatically enabled for all storage accounts and access tiers. Encryption are typically managed by Microsoft, however users can manage their own keys in either (or both) of these two ways: \cite{blob_at_rest,blob_keys1,blob_keys2}

\begin{itemize}
    \item [--] \textit{Customer-managed keys}, which can be stored using some of Azure's key management solutions (e.g. Azure Vault), and they can be created by the customer or by the Azure Key Vault APIs.

    \item [--] \textit{Customer-provided keys}, which are encryption keys that the client provides on requests to Blob storage. This key is discarded as soon as the operation completes, and the same key must be used for subsequent operations on that blob.
\end{itemize}

A unique feature of Azure Storage is the option for \textit{double encryption}, which involves encryption at both service and infrastructure levels, with different keys and algorithms. This protects against potential compromises of algorithms or keys, making it ideal for customers that required higher security for their data. For encryption at the infrastructure level, the keys are always managed by Microsoft. \cite{blob_at_rest}

However, using Azure Storage Accounts also involves the security of two 256-bit storage account keys which are used for authorizing access to data in the account. Microsoft heavily emphasizes their importance and recommends using Azure Key Vault for their management and frequent rotation (instead of manual management), and urges customers no never distribute them or store them anywhere where they can be easily accessed. \cite {blob_account_keys}

\subsubsection{Access Control}
To control access to its resources, S3 employs many mechanisms, which include: \cite{s3faq, s3_access,s3_access_guidelines,s3_scp,s3_vpc,s3vsblobsecurity}

\begin{description}
    \item[\ac{iam}] can be used to manage the access that users have to S3 resources. IAM users, groups and roles can be created, and policies can be attached to them specifying the level of access they have to any resource. Extra security can be added with \ac{mfa}, requiring for example a code generated by a software app in addition to regular credentials.

    \item[\ac{acl}] are examples of resource-based policies, since they are attached to the S3 resources. Each resource (bucket,object) has an ACL attached to it which lists what AWS accounts have what permissions. Some restrictions do exist, for example permission can only be granted to other AWS accounts, and Amazon actually recommends disabling ACLs and instead simplifying management by automatically having the bucket owner also own all objects in the bucket (regardless of whoever uploaded it). Instead, ACL should only be used fo specific cases where individual access to each object needs to be configured.

    \item[Bucket policies] also are resource-based policies that can be added by the bucket owner, specifying the permissions that other AWS accounts and IAM users have for accessing bucket contents. Bucket policies can replace ACL, and they are necessary for cross-account permissions as ACL doesn't cover every permission, while user policies are only for users in the same Account as the bucket.

    \item[\ac{scp}] allow control over the maximum permissions for all organizational accounts. They are supplementary as they do not actually set permissions, but merely set limits that all accounts must follow to stay within organizational guidelines.

    \item[S3 Block Public Access] provides a centralized way for controlling public access to resources,overriding any potential change to permissions that users may have made, hence strengthening security.

    \item[Query String Authentication] involves the creation of a limited-time URL for accessing an object.

    \item [\ac{vpc}] enable the launch of resources into a virtual network that the customer defines, and VPC endpoints are VPC entities which can offer direct (without internet) connectivity to S3. Access to S3 data can then be controlled by either controlling the users, groups etc. that can use an endpoint, or by controlling which endpoints can access the buckets (using bucket policies).
\end{description}

Blob Storage offers its own range of options for authorizing data access: \cite{blob_authorization,blob_endpoint}

\begin{description}
    \item[A Shared Key] could be used by a client, passing a timestamp and authorization header with every request. This method is not recommended, and users are urged to disable it.

    \item[\ac{sas}] involve the use of a limited-time signed URL which grants controlled access to resources.

    \item[Azure \ac{ad}] is the recommended authorization method by Microsoft, providing centralized, secure and easy access control for resources across many Azure services. Azure \ac{rbac} can be used to manage a user's, group's etc. access to blobs by assigning them a role, while \ac{abac} builds on this to add conditions to these roles. Azure AD also supports MFA and passwordless authentication.

    \item[Anonymous public read access] can be toggled on to allows data to be read without authorization but it can also be disallowed, in which case authorization becomes mandatory.

    \item[Private endpoints] can additionally be used, so that traffic between clients and the storage account goes through Microsoft's network and not through the public internet.
\end{description}

Taking everything into consideration, both S3 and Blob Storage have made great efforts for enforcing access control, however S3 offers a bit more in terms of options as for example Blob Storage does not support resource-based policies.

\subsection{Data Protection}
To protect data from accidental deletions or modifications, and to recover from any such accidents, S3 uses the following mechanisms: \cite{s3faq}

\begin{enumerate}
    \item \textit{Versioning}, once enabled for a bucket, preserves all object versions and allows for easy recovery from accidental deletions and overwrites, by rolling back to any previous version. Versioning can be used in conjunction with S3 lifecycle rules to maintain a rollback window for objects and, for example, store older object versions in a cheaper storage class and automatically delete them after a specified time period has elapsed. This cuts down on costs while also providing a recovery window to previous versions. for even further data protection, MFA Delete can be enabled to add another layer of security, requiring MFA on top of account credentials in order to permanently delete an object version.

    \item \textit{S3 Object Lock} prevents object version deletion and overwrite until a specified amount of time has passed, or until a legal hold is lifted, regardless of storage class or lifecycle transitions used. Object Lock can be enabled at the bucket or object level, and helps in complying with regulations enforcing  \ac{worm} data protection.
\end{enumerate}


Azure Blob also provides comparable data protection options:\cite{blob_data_protection}

\begin{enumerate}
    \item \textit{Storage account locks} can prevent accounts from being deleted (CannotDelete lock)  and their configurations from being modified (ReadOnly lock)

    \item \textit{Immutability policies} can be applied to data, maintaining them in a WORM state for a specified interval or until a legal hold is removed. Immutability policies can be applied at the version-level, but only after a default policy has been configured at the account- or container-level. Immutability policies can also be applied at the container-level, in which case the same policy applies for all objects in that container.

    \item \textit{Soft delete} retains blobs or containers that have been deleted or overwritten for a specified time period, after which the deletion is permanent.

    \item \textit{Blob versioning} maintains previous states (versions) of a blob, which can be used for recovery purposes.  \textit{Point-in-time restore} is similar to versioning, but is only used for block blobs. A cheaper alternative is \textit{blob snapshots}, which allow users to manually saving states when needed, hence cutting down on costs, however \textit{snapshots }are deleted when the objects themselves are deleted.
\end{enumerate}

\subsection{Performance}
Regarding performance, Microsoft has published some performance targets for Blob storage. \cite{blob_targets} While they are high-end, these targets can be achieved keeping in mind that the observed performance always depends on various factors like workload type, object size and more. Some key performance targets are shown in table \ref{Blob targets}

\begin{table}[h]
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{|l|l|}
            \hline
            \multicolumn{1}{|c|}{\textbf{Resource}}                   & \multicolumn{1}{c|}{\textbf{Target}}                     \\ \hline
            Maximum container size                                       & Same as maximum storage account capacity (default 5 PiB) \\ \hline
            \multicolumn{1}{|c|}{Maximum number of blobs and containers} & Unlimited                                                \\ \hline
            Maximum block blob size                                      & 50,000 X 4000 MiB ($\sim$190 TiB)                        \\ \hline
            Blob request rate                                            & Up to 500 requests per second                            \\ \hline
            Maximum blob size via PUT operation                          & 5000 MiB                                                 \\ \hline
            Maximum ingress in LRS/GRS/ZRS regions                       & 60 Gbps                                                  \\ \hline
            Maximum egress in LRS/GRS/ZRS regions                        & 120 Gbps                                                 \\ \hline
            Maximum ingress in other regions                             & 25 Gbps                                                  \\ \hline
            Maximum egress in other regions                              & 50 Gbps                                                  \\ \hline
        \end{tabular}%
    }\caption{\label{Blob targets}Blob Storage Performance Targets}
\end{table}

Amazon hasn't made any performance targets official, however the maximum size of an object can range fro 0 bytes up to 5 TB. A single PUT operation, meanwhile, can upload an object that is 5GB in size at most. Furthermore, the maximum number of buckets in an account is 100 by default, and this limit can be increased to 1000 by paying a service fee. There is no maximum bucket size limit. \cite{s3faq, s3_limits}

The work done in \cite{s3vsblob_performance} shines more light on the performance of cloud storage solutions including S3, S3 Glacier, Blob Storage, as various generated workloads (differing in file size and number) were used to measure and compare the actual performance of each service during 2013-2014.

\subsubsection{Latency}
The authors in \cite{s3vsblob_performance} calculated the \ac{rtt} experienced when using cloud storage services, as shown in figure \ref{fig:latency}
\begin{figure} [h]
    \centering
    \includegraphics[scale=0.7]{images/latency}
    \caption{\label{fig:latency}Latency of cloud services}
\end{figure}


It was established from the results that RTTs are stable and nearly identical for S3 and Azure services (in the same regions), and the determining factor was propagation delay (due to distance from data center).

\subsubsection{Throughput}
Research was also conducted in \cite{s3vsblob_performance} to measure throughput when downloading single files and bundle of files. Results are in figure  \ref{fig:throughput} .
\begin{figure} [h]
    \centering
    \includegraphics[scale=0.5]{images/throughput}
    \caption{\label{fig:throughput}Download throughput of cloud services}
\end{figure}

Across all services, performance improves for large files but this is a natural result of \ac{tcp} algorithms, and Glacier is by far the slowest service (with strict download limits set), which is natural as it is meant for archiving. In general, however, there is no clear winner as Azure performs better for small files, while S3 comes on top for large files, and there also exist performance differences for each service's data centers, which is probably a result of differences in connection quality. In addition,  the results are similar when it comes to downloading bundles of files, though overall throughput is vastly reduced due to the overhead involved in managing the vast number of files .

\subsubsection{Performance Variation}
Another benchmark in \cite{s3vsblob_performance} was carried out to determine if the performance of each services varies due to, for example peak traffic causing slowdown at certain points in the day.
\begin{figure} [h]
    \centering
    \includegraphics[scale=0.5]{images/variation}
    \caption{\label{fig:variation}Performance variation of cloud services}
\end{figure}

As shown in \ref{fig:variation}, upload speeds remain almost perfectly stable across all providers. Download speeds, on the other hand, vary wildly depending on the service, data center and workload. Glacier speeds remain capped as mentioned earlier, while Azure performs the best (likely due to the small workload size). S3 in Dublin shows the greatest variation, frequently jumping between 20 and 100 Mb/s, due to the great difference in performance capability of the server group that is reached each time (one is limited to 20 Mb/s).

\subsubsection{Connection Management and Overhead}
The way each service manages its connection when uploading multiple files also affects the amount of overhead that is generated each time. Amazon offers the most cleverly implemented services in this regard, since S3 uses two connections, one for user login and another for data transfer, while Glacier uses only a single connection for both (regardless of the number of uploaded files). Azure, however, uses a separate connection for each transferred file, and so it suffers from especially increased overhead when many small files are uploaded. (figure \ref{fig:overhead})

\begin{figure} [h]
    \centering
    \includegraphics[scale=0.3]{images/overhead}
    \caption{\label{fig:overhead}Upload overhead of cloud services}
\end{figure}

\subsection{Pricing}
Azure Blob and S3 both are based on a "pay-as-you-go" charging model,  and the results for some comparable storage classes are shown below. The prices shown in \ref{tab:object_pricing} are for the US East (N.Virginia) S3 region and East US 2 Blob region. Blob Storage also takes into account the redundancy method used when presenting the prices and, for Blob, the flat structure namespace option was selected. \cite{s3_pricing,blob_pricing}

\begin{table}[!h]
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{|llll|}
            \hline
            \multicolumn{1}{|l|}{} &
            \multicolumn{1}{l|}{\textbf{Amazon S3-IA}} &
            \multicolumn{1}{l|}{\textbf{Blob Cool Tier LRS}} &
            \textbf{Blob Cool Tier GRS} \\ \hline
            \multicolumn{4}{|c|}{\cellcolor[HTML]{EFEFEF}\textbf{Data Storage}} \\ \hline
            \multicolumn{1}{|l|}{Per Month Storage} &
            \multicolumn{1}{l|}{\$0.0125 per GB} &
            \multicolumn{1}{l|}{\$0.01 per GB} &
            \$0.02 per GB \\ \hline
            \multicolumn{4}{|c|}{\cellcolor[HTML]{EFEFEF}\textbf{Data Transfer}} \\ \hline
            \multicolumn{1}{|l|}{Data Retrieval} &
            \multicolumn{1}{l|}{\$0.01 per GB} &
            \multicolumn{1}{l|}{\$0.01 per GB} &
            \$0.01 per GB \\ \hline
            \multicolumn{1}{|l|}{Transfer IN from Internet} &
            \multicolumn{1}{l|}{Free} &
            \multicolumn{1}{l|}{Free} &
            Free \\ \hline
            \multicolumn{1}{|l|}{Transfer TO another Region/AZ} &
            \multicolumn{1}{l|}{$0.01 - $0.02 per GB} &
            \multicolumn{1}{l|}{\$0.01 per GB} &
            \$0.01 per GB \\ \hline
            \multicolumn{4}{|c|}{\cellcolor[HTML]{EFEFEF}\textbf{Requests}} \\ \hline
            \multicolumn{1}{|l|}{Write Requests} & \multicolumn{1}{l|}{\$0.01 per 1,000 requests}  & \multicolumn{1}{l|}{\$0.1 per 10,000 requests}   & \$0.2 per 10,000 requests   \\ \hline
            \multicolumn{1}{|l|}{Read Requests}  & \multicolumn{1}{l|}{\$0.001 per 1,000 requests} & \multicolumn{1}{l|}{\$0.01 per 10,000 requests}  & \$0,01 per 10,000 requests  \\ \hline
            \multicolumn{1}{|l|}{Delete} &
            \multicolumn{1}{l|}{Free} &
            \multicolumn{1}{l|}{Free} &
            Free \\ \hline
            \multicolumn{1}{|l|}{Other Requests} & \multicolumn{1}{l|}{\$0.001 per 1,000 requests} & \multicolumn{1}{l|}{\$0.004 per 10,000 requests} & \$0.004 per 10,000 requests \\ \hline
        \end{tabular}%
    }\caption{\label{tab:object_pricing}Object Storage Pricing Sample}
\end{table}

It can be observed from \ref{tab:object_pricing} that, in general, S3 and Blob prices are similar, with Blob Cool Tier LRS overall being a bit cheaper than S3-IA, and GRS being the most expensive fo the three. However, this is only a sample, and the cost-efficiency of each service can only be determined after extensive analysis and careful consideration of various factors like the storage classes and redundancy options selected, workloads to be carried out and more.

One interesting discovery was made in \cite{s3vsblob_performance}, which was that, while S3's expected charged are more or less identical to actual charges, Blob's actual charges in bandwidth and storage are 25\% cheaper than expected.


\section{Conclusions}
